{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c44042-8cbd-4cbe-ba89-396150be344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Dropout, Embedding, RepeatVector, concatenate,TimeDistributed\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "# import tensorflow.keras.utils\n",
    "import tensorflow.keras.utils as np_utils\n",
    "from time import time\n",
    "import argparse\n",
    "import joblib\n",
    "import re\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606f50bd-22ef-4424-9f68-66cdc93d3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7c3528-ad04-4679-9b65-67e470fe1f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--max_vocab_size MAX_VOCAB_SIZE] [--max_seq_len MAX_SEQ_LEN]\n",
      "                             [--embedding_dim EMBEDDING_DIM] [--hidden_state_dim HIDDEN_STATE_DIM] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--learning_rate LEARNING_RATE] [--dropout DROPOUT]\n",
      "                             [--data_path DATA_PATH] [--outpath OUTPATH] [--version VERSION] [--mode MODE]\n",
      "                             [--num_train_records NUM_TRAIN_RECORDS] [--load_model_from LOAD_MODEL_FROM]\n",
      "                             [--vocabulary_path VOCABULARY_PATH] [--reverse_vocabulary_path REVERSE_VOCABULARY_PATH]\n",
      "                             [--count_vectorizer_path COUNT_VECTORIZER_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Jessy Francisco\\AppData\\Roaming\\jupyter\\runtime\\kernel-8c67bc65-f02b-4844-8fdb-f4cdcc5ee648.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class TwitterBot:\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser(description='Process the inputs')\n",
    "        parser.add_argument('--max_vocab_size',help='maximum words in the vocabulary',default=50000)\n",
    "        parser.add_argument('--max_seq_len',help='maximum words to process in tweet',default=30)\n",
    "        parser.add_argument('--embedding_dim',help='maximum words to process in tweet',default=100)\n",
    "        parser.add_argument('--hidden_state_dim',help='hidden dimension of the LSTM',default=100)\n",
    "        parser.add_argument('--epochs',help='Number of epochs to training',default=100)\n",
    "        parser.add_argument('--batch_size',help='Batch size for training',default=30)\n",
    "        parser.add_argument('--learning_rate',help='Learning rate for training',default=1e-4)\n",
    "        parser.add_argument('--dropout',help='dropout',default=None)\n",
    "        parser.add_argument('--data_path',help='Path for the training dataset')\n",
    "        parser.add_argument('--outpath',help='output directory')\n",
    "        parser.add_argument('--version', help='version run of the code',default='v1')\n",
    "        parser.add_argument('--mode',help='train/inference',default='train')\n",
    "        parser.add_argument('--num_train_records',help='no of records to use for training',default=20000)\n",
    "        parser.add_argument('--load_model_from',help='saved model path')\n",
    "        parser.add_argument('--vocabulary_path',help='vocabulary path')\n",
    "        parser.add_argument('--reverse_vocabulary_path',help='reverse vocabulary')\n",
    "        parser.add_argument('--count_vectorizer_path',help='count_vectorizer_path')\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        self.max_vocab_size = int(args.max_vocab_size)\n",
    "        self.max_seq_len = int(args.max_seq_len )\n",
    "        self.embedding_dim = int(args.embedding_dim)\n",
    "        self.hidden_state_dim = int(args.hidden_state_dim)\n",
    "        self.epochs = int(args.epochs)\n",
    "        self.batch_size = int(args.batch_size)\n",
    "        self.dropout= float(args.dropout)\n",
    "        self.learning_rate = float(args.learning_rate)\n",
    "        self.UNK = 0\n",
    "        self.PAD = 1\n",
    "        self.START = 2\n",
    "        self.data_path = args.data_path\n",
    "        self.outpath  = args.outpath\n",
    "        self.version = args.version\n",
    "        self.mode = args.mode\n",
    "        self.num_train_records = int(args.num_train_records)\n",
    "        self.load_model_from = args.load_model_from\n",
    "        self.vocabulary_path = args.vocabulary_path\n",
    "        self.reverse_vocabulary_path = args.reverse_vocabulary_path\n",
    "        self.count_vectorizer_path  = args.count_vectorizer_path\n",
    "\n",
    "    def process_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            data = pd.read_csv(path)\n",
    "            data['in_response_to_tweet_id'].fillna(-12345, inplace=True)\n",
    "            tweets_in = data[data['in_response_to_tweet_id'] == -12345]\n",
    "            tweets_in_out = tweets_in.merge(data, left_on=['tweet_id'], right_on=['in_response_to_tweet_id'])\n",
    "            return tweets_in_out[:self.num_train_records]\n",
    "        elif self.mode == 'inference':\n",
    "            return data\n",
    "\n",
    "    def replace_anonymized_names(self,data):\n",
    "\n",
    "        def replace_name(match):\n",
    "            cname = match.group(2).lower()\n",
    "            if not cname.isnumeric():\n",
    "                return match.group(1) + match.group(2)\n",
    "            return '@__cname__'\n",
    "        \n",
    "        re_pattern = re.compile('(\\W@|^@)([a-zA-Z0-9_]+)')\n",
    "        #print(data['text_x'])\n",
    "        if self.mode == 'train':\n",
    "\n",
    "            in_text = data['text_x'].apply(lambda txt:re_pattern.sub(replace_name,txt))\n",
    "            out_text = data['text_y'].apply(lambda txt:re_pattern.sub(replace_name,txt))\n",
    "            return list(in_text.values),list(out_text.values)\n",
    "        else:\n",
    "            return list(map(lambda x:re_pattern.sub(replace_name,x),data))\n",
    "\n",
    "    def tokenize_text(self,in_text,out_text):\n",
    "        count_vectorizer = CountVectorizer(tokenizer=casual_tokenize, max_features=self.max_vocab_size - 3)\n",
    "        count_vectorizer.fit(in_text + out_text)\n",
    "        self.analyzer = count_vectorizer.build_analyzer()\n",
    "        self.vocabulary = {key_: value_ + 3 for key_,value_ in count_vectorizer.vocabulary_.items()}\n",
    "        self.vocabulary['UNK'] = self.UNK\n",
    "        self.vocabulary['PAD'] = self.PAD\n",
    "        self.vocabulary['START'] = self.START\n",
    "        self.reverse_vocabulary = {value_: key_ for key_, value_ in self.vocabulary.items()}\n",
    "        joblib.dump(self.vocabulary,self.outpath + 'vocabulary.pkl')\n",
    "        joblib.dump(self.reverse_vocabulary,self.outpath + 'reverse_vocabulary.pkl')\n",
    "        joblib.dump(count_vectorizer,self.outpath + 'count_vectorizer.pkl')\n",
    "        #pickle.dump(self.count_vectorizer,open(self.outpath + 'count_vectorizer.pkl',\"wb\"))\n",
    "\n",
    "\n",
    "    def words_to_indices(self, sent):\n",
    "        word_indices = [self.vocabulary.get(token, self.UNK) for token in self.analyzer(sent)] + [self.PAD] * self.max_seq_len\n",
    "        word_indices = word_indices[:self.max_seq_len]\n",
    "        return word_indices\n",
    "\n",
    "    def indices_to_words(self,indices):\n",
    "        return ' '.join(self.reverse_vocabulary[id] for id in indices if id != self.PAD).strip() \n",
    "    \n",
    "\n",
    "    def data_transform(self,in_text,out_text):\n",
    "            #rint(in_text),print(out_text)\t\n",
    "            X = [self.words_to_indices(s) for s in in_text]\n",
    "            Y = [self.words_to_indices(s) for s in out_text]\n",
    "            #X = lambda sent:self.words_to_indices(sent))\n",
    "            #Y = out_text.apply(lambda sent:self.words_to_indices(sent))\n",
    "            return np.array(X),np.array(Y)\n",
    "\n",
    "    def train_test_split_(self,X,Y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=0)\n",
    "        #X_train = X_train[:,:,np.newaxis]\n",
    "        #X_test = X_test[:,:,np.newaxis]\n",
    "        y_train = y_train[:,:,np.newaxis]\n",
    "        y_test = y_test[:,:,np.newaxis]\t\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def data_creation(self):\n",
    "            data = self.process_data(self.data_path)\n",
    "            #print(data.head()) \n",
    "            in_text,out_text =  self.replace_anonymized_names(data)\n",
    "            test_sentences = []\n",
    "            test_indexes= np.random.randint(1,self.num_train_records,10)\n",
    "            for ind in test_indexes:\n",
    "                sent = in_text[ind]\n",
    "                test_sentences.append(sent)\n",
    "        #print(in_text[:10])\n",
    "            #rint(in_text.shape,out_text.shape)\n",
    "            self.tokenize_text(in_text,out_text)\n",
    "            X,Y = self.data_transform(in_text,out_text)\n",
    "            X_train, X_test, y_train, y_test = self.train_test_split_(X,Y)\n",
    "            return X_train, X_test, y_train, y_test,test_sentences\n",
    "\n",
    "\n",
    "    def define_model(self):\n",
    "        \n",
    "        # Embedding Layer\n",
    "        embedding = Embedding(\n",
    "            output_dim=self.embedding_dim,\n",
    "            input_dim=self.max_vocab_size,\n",
    "            input_length=self.max_seq_len,\n",
    "            name='embedding',\n",
    "        )\n",
    "        \n",
    "        # Encoder input\n",
    "    \n",
    "        encoder_input = Input(\n",
    "            shape=(self.max_seq_len,),\n",
    "            dtype='int32',\n",
    "            name='encoder_input',\n",
    "        )\n",
    "        \n",
    "        embedded_input = embedding(encoder_input)\n",
    "\n",
    "    \n",
    "        encoder_rnn = LSTM(\n",
    "            self.hidden_state_dim,\n",
    "            name='encoder',\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "        # Context is repeated to the max sequence length so that the same context \n",
    "        # can be feed at each step of decoder\n",
    "        context = RepeatVector(self.max_seq_len)(encoder_rnn(embedded_input))\n",
    "    \n",
    "        # Decoder    \n",
    "        last_word_input = Input(\n",
    "            shape=(self.max_seq_len,),\n",
    "            dtype='int32',\n",
    "            name='last_word_input',\n",
    "        )\n",
    "        \n",
    "        embedded_last_word = embedding(last_word_input)\n",
    "        # Combines the context produced by the encoder and the last word uttered as inputs\n",
    "        # to the decoder.\n",
    "        \n",
    "        decoder_input = concatenate([embedded_last_word, context],axis=2)\n",
    "\n",
    "        # return_sequences causes LSTM to produce one output per timestep instead of one at the\n",
    "        # end of the intput, which is important for sequence producing models.\n",
    "        decoder_rnn = LSTM(\n",
    "            self.hidden_state_dim,\n",
    "            name='decoder',\n",
    "            return_sequences=True,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "        \n",
    "        decoder_output = decoder_rnn(decoder_input)\n",
    "    \n",
    "        # TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n",
    "        next_word_dense = TimeDistributed(\n",
    "            Dense(int(self.max_vocab_size/20),activation='relu'),\n",
    "            name='next_word_dense',\n",
    "        )(decoder_output)\n",
    "        \n",
    "        next_word = TimeDistributed(\n",
    "            Dense(self.max_vocab_size,activation='softmax'),\n",
    "            name='next_word_softmax'\n",
    "        )(next_word_dense)\n",
    "        \n",
    "        return Model(inputs=[encoder_input,last_word_input], outputs=[next_word])\n",
    "        \n",
    "    \n",
    "\n",
    "\t\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    def respond_to_input(self,model,input_sent):\n",
    "        input_y = self.include_start_token(self.PAD * np.ones((1,self.max_seq_len)))\n",
    "        ids = np.array(self.words_to_indices(input_sent)).reshape((1,self.max_seq_len))\n",
    "        for pos in range(self.max_seq_len -1):\n",
    "            pred = model.predict([ids, input_y]).argmax(axis=2)[0]\n",
    "            #pred = model.predict([ids, input_y])[0]\n",
    "            input_y[:,pos + 1] = pred[pos]\n",
    "        return self.indices_to_words(model.predict([ids,input_y]).argmax(axis=2)[0])\n",
    "\n",
    "    def create_model(self):\n",
    "\t    _model_ = self.define_model()\n",
    "\t    adam = Adam(lr=self.learning_rate,clipvalue=5.0)\n",
    "\t    _model_.compile(optimizer=adam,loss='sparse_categorical_crossentropy')\n",
    "\t    return _model_\n",
    "\n",
    "    def include_start_token(self,Y):\n",
    "        print(Y.shape)\n",
    "        Y = Y.reshape((Y.shape[0],Y.shape[1]))\n",
    "        Y = np.hstack((self.START*np.ones((Y.shape[0],1)),Y[:, :-1]))\n",
    "        #Y = Y[:,:,np.newaxis]\t\n",
    "        return Y\n",
    "\n",
    "    def binarize_output_response(self,Y):\n",
    "\t    return np.array([np_utils.to_categorical(row, num_classes=self.max_vocab_size)\n",
    "\t\tfor row in Y])\n",
    "\n",
    "    def train_model(self,model,X_train,X_test,y_train,y_test):\n",
    "        input_y_train = self.include_start_token(y_train)\n",
    "        print(input_y_train.shape)\n",
    "        input_y_test = self.include_start_token(y_test)\n",
    "        print(input_y_test.shape)\n",
    "        early = EarlyStopping(monitor='val_loss',patience=10,mode='auto')\n",
    "\n",
    "        checkpoint = ModelCheckpoint(self.outpath + 's2s_model_' + str(self.version) + '_.h5',monitor='val_loss',verbose=1,save_best_only=True,mode='auto')\n",
    "        lr_reduce = ReduceLROnPlateau(monitor='val_loss',factor=0.5, patience=2, verbose=0, mode='auto')\n",
    "        model.fit([X_train,input_y_train],y_train, \n",
    "\t\t      epochs=self.epochs,\n",
    "\t\t      batch_size=self.batch_size, \n",
    "\t\t      validation_data=[[X_test,input_y_test],y_test], \n",
    "\t\t      callbacks=[early,checkpoint,lr_reduce], \n",
    "\t\t      shuffle=True)\n",
    "        return model\n",
    "\n",
    "    def generate_response(self,model,sentences):\n",
    "        output_responses = []\n",
    "        print(sentences)\n",
    "        for sent in sentences:\n",
    "            response = self.respond_to_input(model,sent)\n",
    "            output_responses.append(response)\n",
    "        out_df = pd.DataFrame()\n",
    "        out_df['Tweet in'] = sentences\n",
    "        out_df['Tweet out'] = output_responses\n",
    "        return out_df\n",
    "\n",
    "    def main(self):\n",
    "            if self.mode == 'train':\n",
    "\n",
    "                X_train, X_test, y_train, y_test,test_sentences = self.data_creation()\n",
    "                print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n",
    "                print('Data Creation completed')\n",
    "                model = self.create_model()\n",
    "                print(\"Model creation completed\")\n",
    "                model = self.train_model(model,X_train,X_test,y_train,y_test)\n",
    "                test_responses = self.generate_response(model,test_sentences)\n",
    "                print(test_sentences)  \n",
    "                print(test_responses)\n",
    "                pd.DataFrame(test_responses).to_csv(self.outpath + 'output_response.csv',index=False)\n",
    "\n",
    "            elif self.mode == 'inference':\n",
    "\n",
    "                model = load_model(self.load_model_from)\n",
    "                self.vocabulary = joblib.load(self.vocabulary_path)\n",
    "                self.reverse_vocabulary = joblib.load(self.reverse_vocabulary_path)\n",
    "                #nalyzer_file = open(self.analyzer_path,\"rb\")\n",
    "                count_vectorizer = joblib.load(self.count_vectorizer_path)\n",
    "                self.analyzer = count_vectorizer.build_analyzer()\n",
    "                data = self.process_data(self.data_path)\n",
    "                col = data.columns.tolist()[0]\n",
    "                test_sentences = list(data[col].values)\n",
    "                test_sentences = self.replace_anonymized_names(test_sentences)\n",
    "                responses = self.generate_response(model,test_sentences)\n",
    "                print(responses)\n",
    "                responses.to_csv(self.outpath + 'responses_' + str(self.version) + '_.csv',index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time()\n",
    "    obj = TwitterBot()\n",
    "    obj.main()\n",
    "    end_time = time()\n",
    "    print(\"Processing finished, time taken is %s\",end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cedf4-ea5d-4e02-8027-b06e9ea3fa89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
